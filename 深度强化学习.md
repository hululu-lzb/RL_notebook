# 1、多臂老虎机

## 贪心算法

![截屏2021-10-30 下午7.32.27](/Users/liuzhibin/Documents/截屏2021-10-30 下午7.32.27.png)

公式中的预期最大是从上帝视角看的，所以是先验已知的。同时这里因为每个的奖励都是单位一，所以直接概率相减了。

![截屏2021-10-30 下午7.36.43](/Users/liuzhibin/Documents/截屏2021-10-30 下午7.36.43.png)

这里实际上是从电脑的角度来看，用estimate去逐渐估计真实的价值（因为奖励是单位一所以会逐渐和之前的概率相等）。

![截屏2021-10-30 下午7.39.59](/Users/liuzhibin/Documents/截屏2021-10-30 下午7.39.59.png)

这两个是不同的角度，前者是上帝视角看，然后用于评价学习的效果。后者是通过学习逐渐逼近前者。

## 上置信届算法

除了考虑每个拉杆的期望奖励外，同时还考虑了拉杆的不确定性。拉杆被拉的次数越多，不确定性越小。

![截屏2021-11-02 下午7.10.06](/Users/liuzhibin/Documents/截屏2021-11-02 下午7.10.06.png)

## 汤普森采样

利用样本估计总体

# 2、马尔可夫决策过程

## 马尔可夫性质

一个随机过程被称为具有马尔可夫性质，当且仅当某时刻的状态只却决于上一时刻的状态。

## 马尔可夫过程

马尔可夫过程指具有马尔可夫性质的随机过程，也被称为马尔可夫链。

![截屏2021-11-03 下午4.53.59](/Users/liuzhibin/Documents/截屏2021-11-03 下午4.53.59.png)

![截屏2021-11-03 下午4.55.08](/Users/liuzhibin/Documents/截屏2021-11-03 下午4.55.08.png)

## 马尔可夫奖励过程

在之前基础上加上了奖励和折扣

## 回报

![截屏2021-11-03 下午4.58.27](/Users/liuzhibin/Documents/截屏2021-11-03 下午4.58.27.png)

从某一个状态开始到终止状态的所有奖励可以写为上市，即St状态的奖励。某个状态的奖励不仅与当前状态奖励有关还与之后的状态奖励有关。

## 价值函数

价值函数相当于一个状态的期望回报

![截屏2021-11-03 下午5.08.14](/Users/liuzhibin/Documents/截屏2021-11-03 下午5.08.14.png)

即使奖励等于即使奖励的期望是论文已经证明的。

s‘是从s到下一个状态的很多可能。用条件概率乘上相应的价值就相当于求均值。

## 马尔可夫决策过程

在马尔可夫奖励过程上加上动作就是马尔可夫决策过程

![截屏2021-11-04 下午4.50.20](/Users/liuzhibin/Documents/截屏2021-11-04 下午4.50.20.png)

之前是到达一个状态后直接得到奖励，现在还取决于动作。

## 策略

就是在每个状态采取每个动作的概率

![截屏2021-11-04 下午4.59.43](/Users/liuzhibin/Documents/截屏2021-11-04 下午4.59.43.png)

状态价值就是将所有可能采取的动作值求均值。动作还要考虑到下一个。

## 贝尓曼期望方程

![截屏2021-11-04 下午5.24.47](/Users/liuzhibin/Documents/截屏2021-11-04 下午5.24.47.png)

## 求解MDP的价值函数的方法

1、转化法：求解的时候关键是将带动作的状态奖励求均值，和带动作的状态转移函数转化为不带动作的。即转化为MRP。

![截屏2021-11-04 下午5.26.09](/Users/liuzhibin/Documents/截屏2021-11-04 下午5.26.09.png)

2、蒙特卡洛方法：通过采样很多次序列，之后取平均值来计算价值函数。

# 3、动态规划算法

强化学习算法主要有两部分：策略迭代和价值迭代。强化学习中环境就是要求解的问题，然后奖励函数和状态转移函数是已知的，目标就是要得到该环境下的最佳策略。

## 策略迭代

包括策略评估和策略提升。策略评估就是利用马尔可夫决策过程中的贝尔曼期望方程求出状态价值函数。求的时候要进行多轮实验，利用上一轮的第k+1个状态来求解当前轮的k状态。

![截屏2021-11-04 下午7.42.14](/Users/liuzhibin/Documents/截屏2021-11-04 下午7.42.14.png)

策略提升就是直接选择当前状态下最大的动作价值函数，如果只有一个动作的动作价值函数是最大的，那么策略中这个动作的概率直接被设置为1（如果有多个就1/个数均分），其他的为零。

## 价值迭代

相当于只进行一次策略评估的策略迭代。策略迭代是使用当前状态所有动作的均值进行更新，而价值迭代是使用最大动作奖励进行更新。可以这样做的理由：虽然价值函数可能未收敛，但是可能策略更新已经收敛了，即最大奖励的动作已经收敛了，只是奖励的具体值依旧在改变。

